{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65b3613",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:50.064250Z",
     "iopub.status.busy": "2022-03-12T18:15:50.062504Z",
     "iopub.status.idle": "2022-03-12T18:15:50.079847Z",
     "shell.execute_reply": "2022-03-12T18:15:50.080360Z",
     "shell.execute_reply.started": "2022-03-12T16:43:11.064930Z"
    },
    "papermill": {
     "duration": 0.038261,
     "end_time": "2022-03-12T18:15:50.080601",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.042340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/penguins/penguins.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774ff32e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:50.116024Z",
     "iopub.status.busy": "2022-03-12T18:15:50.115441Z",
     "iopub.status.idle": "2022-03-12T18:15:50.136705Z",
     "shell.execute_reply": "2022-03-12T18:15:50.136241Z",
     "shell.execute_reply.started": "2022-03-12T16:51:22.574317Z"
    },
    "papermill": {
     "duration": 0.039673,
     "end_time": "2022-03-12T18:15:50.136826",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.097153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset excluding null values\n",
    "penguins  = pd.read_csv(\"../input/penguins/penguins.csv\").dropna()\n",
    "\n",
    "# Deep Learning Models work best when features are on a similar scale\n",
    "# we'll just rescale the FlipperLength and BodyMass so they're on a similar scale to the bill measurements\n",
    "penguins['FlipperLength'] = penguins['FlipperLength']/10\n",
    "penguins['BodyMass'] = penguins['BodyMass']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b81d87ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:50.168678Z",
     "iopub.status.busy": "2022-03-12T18:15:50.167871Z",
     "iopub.status.idle": "2022-03-12T18:15:50.185031Z",
     "shell.execute_reply": "2022-03-12T18:15:50.185442Z",
     "shell.execute_reply.started": "2022-03-12T16:53:22.471834Z"
    },
    "papermill": {
     "duration": 0.034077,
     "end_time": "2022-03-12T18:15:50.185568",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.151491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CulmenLength</th>\n",
       "      <th>CulmenDepth</th>\n",
       "      <th>FlipperLength</th>\n",
       "      <th>BodyMass</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>45.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>21.5</td>\n",
       "      <td>47.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>41.8</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.8</td>\n",
       "      <td>44.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38.2</td>\n",
       "      <td>18.1</td>\n",
       "      <td>18.5</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>38.6</td>\n",
       "      <td>17.2</td>\n",
       "      <td>19.9</td>\n",
       "      <td>37.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>49.2</td>\n",
       "      <td>15.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>63.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>39.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>19.1</td>\n",
       "      <td>30.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>41.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>40.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>35.6</td>\n",
       "      <td>17.5</td>\n",
       "      <td>19.1</td>\n",
       "      <td>31.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>41.1</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.9</td>\n",
       "      <td>33.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>47.4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>21.2</td>\n",
       "      <td>47.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CulmenLength  CulmenDepth  FlipperLength  BodyMass  Species\n",
       "230          45.2         13.8           21.5     47.50        1\n",
       "69           41.8         19.4           19.8     44.50        0\n",
       "23           38.2         18.1           18.5     39.50        0\n",
       "106          38.6         17.2           19.9     37.50        0\n",
       "169          49.2         15.2           22.1     63.00        1\n",
       "128          39.0         17.1           19.1     30.50        0\n",
       "63           41.1         18.2           19.2     40.50        0\n",
       "136          35.6         17.5           19.1     31.75        0\n",
       "119          41.1         18.6           18.9     33.25        0\n",
       "234          47.4         14.6           21.2     47.25        1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dataset is too small to be useful for deep learning\n",
    "# So we'll oversample it to increase its size\n",
    "for i in range(1,3):\n",
    "    penguins = penguins.append(penguins)\n",
    "\n",
    "# Display a random sample of 10 observations\n",
    "sample = penguins.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a32a3",
   "metadata": {
    "papermill": {
     "duration": 0.012887,
     "end_time": "2022-03-12T18:15:50.211557",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.198670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **Species** column is the label our model will predict. Each label value represents a class of penguin species, encoded as 0, 1, or 2. The following code shows the actual species to which these class labels corrrespond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56422522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:50.245785Z",
     "iopub.status.busy": "2022-03-12T18:15:50.240867Z",
     "iopub.status.idle": "2022-03-12T18:15:50.257694Z",
     "shell.execute_reply": "2022-03-12T18:15:50.256996Z",
     "shell.execute_reply.started": "2022-03-12T17:00:34.650894Z"
    },
    "papermill": {
     "duration": 0.033284,
     "end_time": "2022-03-12T18:15:50.257829",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.224545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CulmenLength' 'CulmenDepth' 'FlipperLength' 'BodyMass' 'Species'] SpeciesName\n",
      "[ 35.9 19.2 18.9 38.0 0 ] Adelie\n",
      "[ 47.5 15.0 21.8 49.5 1 ] Gentoo\n",
      "[ 45.0 15.4 22.0 50.5 1 ] Gentoo\n",
      "[ 46.8 15.4 21.5 51.5 1 ] Gentoo\n",
      "[ 35.0 17.9 19.2 37.25 0 ] Adelie\n",
      "[ 39.7 18.9 18.4 35.5 0 ] Adelie\n",
      "[ 35.9 19.2 18.9 38.0 0 ] Adelie\n",
      "[ 37.3 20.5 19.9 37.75 0 ] Adelie\n",
      "[ 36.8 18.5 19.3 35.0 0 ] Adelie\n",
      "[ 41.4 18.5 20.2 38.75 0 ] Adelie\n"
     ]
    }
   ],
   "source": [
    "penguin_classes = ['Adelie', 'Gentoo', 'Chinstrap']\n",
    "print(sample.columns[0:5].values, 'SpeciesName')\n",
    "for index, row in penguins.sample(10).iterrows():\n",
    "    print('[',row[0], row[1], row[2],row[3], int(row[4]), ']',penguin_classes[int(row[-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b7eec",
   "metadata": {
    "papermill": {
     "duration": 0.013881,
     "end_time": "2022-03-12T18:15:50.286667",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.272786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As is common in a supervised learning problem, we'll split the dataset into a set of records with which to train the model, and a smaller set with which to validate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3516c7c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:50.319696Z",
     "iopub.status.busy": "2022-03-12T18:15:50.319040Z",
     "iopub.status.idle": "2022-03-12T18:15:51.165001Z",
     "shell.execute_reply": "2022-03-12T18:15:51.165589Z",
     "shell.execute_reply.started": "2022-03-12T17:15:01.064514Z"
    },
    "papermill": {
     "duration": 0.865596,
     "end_time": "2022-03-12T18:15:51.165784",
     "exception": false,
     "start_time": "2022-03-12T18:15:50.300188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 957, Test Set: 411 \n",
      "\n",
      "Sample of features and labels:\n",
      "[51.1 16.5 22.5 52.5] 1 (Gentoo)\n",
      "[50.7 19.7 20.3 40.5] 2 (Chinstrap)\n",
      "[49.5 16.2 22.9 58. ] 1 (Gentoo)\n",
      "[39.3 20.6 19.  36.5] 0 (Adelie)\n",
      "[42.5 20.7 19.7 45. ] 0 (Adelie)\n",
      "[50.  15.3 22.  55.5] 1 (Gentoo)\n",
      "[50.2  18.7  19.8  37.75] 2 (Chinstrap)\n",
      "[50.7 19.7 20.3 40.5] 2 (Chinstrap)\n",
      "[49.1  14.5  21.2  46.25] 1 (Gentoo)\n",
      "[43.2 16.6 18.7 29. ] 2 (Chinstrap)\n",
      "[38.8  17.6  19.1  32.75] 0 (Adelie)\n",
      "[37.8 17.1 18.6 33. ] 0 (Adelie)\n",
      "[45.8 14.2 21.9 47. ] 1 (Gentoo)\n",
      "[43.8 13.9 20.8 43. ] 1 (Gentoo)\n",
      "[36.  17.1 18.7 37. ] 0 (Adelie)\n",
      "[43.3 13.4 20.9 44. ] 1 (Gentoo)\n",
      "[36.  18.5 18.6 31. ] 0 (Adelie)\n",
      "[41.1  19.   18.2  34.25] 0 (Adelie)\n",
      "[33.1 16.1 17.8 29. ] 0 (Adelie)\n",
      "[40.9 13.7 21.4 46.5] 1 (Gentoo)\n",
      "[45.2 17.8 19.8 39.5] 2 (Chinstrap)\n",
      "[48.4 14.6 21.3 58.5] 1 (Gentoo)\n",
      "[43.6 13.9 21.7 49. ] 1 (Gentoo)\n",
      "[38.5  17.9  19.   33.25] 0 (Adelie)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "label = 'Species'\n",
    "\n",
    "# Split the 70-30% into training and test datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(penguins[features].values,\n",
    "                                                    penguins[label].values,\n",
    "                                                    test_size = 0.30,\n",
    "                                                    random_state = 0\n",
    "                                                   )\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
    "print(\"Sample of features and labels:\")\n",
    "\n",
    "# Take a look at the first 25 training features and corresponding labels\n",
    "for n in range(0, 24):\n",
    "    print(x_train[n], y_train[n], \"(\" + penguin_classes[y_train[n]] + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0bf71",
   "metadata": {
    "papermill": {
     "duration": 0.013696,
     "end_time": "2022-03-12T18:15:51.194022",
     "exception": false,
     "start_time": "2022-03-12T18:15:51.180326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The *features* are the measurements for each penguin observation, and the *label* is a numeric value that indicates the species of penguin that the observation represents (Adelie, Gentoo, or Chinstrap).\n",
    "\n",
    "## Install and import TensorFlow libraries\n",
    "\n",
    "Since we plan to use TensorFlow to create our penguin classifier, we'll need to run the following two cells to install and import the libraries we intend to use.\n",
    "\n",
    "> **Note** *Keras* is an abstraction layer over the base TensorFlow API. In most common machine learning scenarios, you can use Keras to simplify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "674b6e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:15:51.225671Z",
     "iopub.status.busy": "2022-03-12T18:15:51.224927Z",
     "iopub.status.idle": "2022-03-12T18:16:53.803746Z",
     "shell.execute_reply": "2022-03-12T18:16:53.803007Z",
     "shell.execute_reply.started": "2022-03-12T17:20:24.842027Z"
    },
    "papermill": {
     "duration": 62.596213,
     "end_time": "2022-03-12T18:16:53.803883",
     "exception": false,
     "start_time": "2022-03-12T18:15:51.207670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.6.2)\r\n",
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\r\n",
      "     |████████████████████████████████| 497.5 MB 14 kB/s              \r\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.2)\r\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\r\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\r\n",
      "     |████████████████████████████████| 462 kB 51.4 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.43.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.19.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.1.0)\r\n",
      "Collecting libclang>=9.0.1\r\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\r\n",
      "     |████████████████████████████████| 14.5 MB 45.0 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.1.1)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 51.6 MB/s            \r\n",
      "\u001b[?25hCollecting tensorboard<2.9,>=2.8\r\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\r\n",
      "     |████████████████████████████████| 5.8 MB 51.8 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.5.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.15.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\r\n",
      "Collecting keras<2.9,>=2.8.0rc0\r\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "     |████████████████████████████████| 1.4 MB 48.9 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.1)\r\n",
      "Installing collected packages: tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, tensorflow\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.6.0\r\n",
      "    Uninstalling tensorboard-2.6.0:\r\n",
      "      Successfully uninstalled tensorboard-2.6.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.6.0\r\n",
      "    Uninstalling keras-2.6.0:\r\n",
      "      Successfully uninstalled keras-2.6.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.6.2\r\n",
      "    Uninstalling tensorflow-2.6.2:\r\n",
      "      Successfully uninstalled tensorflow-2.6.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\r\n",
      "tfx-bsl 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\r\n",
      "tfx-bsl 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\r\n",
      "tfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.2, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed keras-2.8.0 libclang-13.0.0 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 tf-estimator-nightly-2.8.0.dev2021122109\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3432cfed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:16:54.174915Z",
     "iopub.status.busy": "2022-03-12T18:16:54.173721Z",
     "iopub.status.idle": "2022-03-12T18:16:57.090873Z",
     "shell.execute_reply": "2022-03-12T18:16:57.091552Z",
     "shell.execute_reply.started": "2022-03-12T17:30:14.472148Z"
    },
    "papermill": {
     "duration": 3.102543,
     "end_time": "2022-03-12T18:16:57.091784",
     "exception": false,
     "start_time": "2022-03-12T18:16:53.989241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "Keras version: 2.8.0\n",
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Set the random seed for reproducability\n",
    "tensorflow.random.set_seed(0)\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "print('Keras version:',keras.__version__)\n",
    "print('TensorFlow version:',tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048fc40e",
   "metadata": {
    "papermill": {
     "duration": 0.286308,
     "end_time": "2022-03-12T18:16:57.563205",
     "exception": false,
     "start_time": "2022-03-12T18:16:57.276897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare the data for TensorFlow\n",
    "\n",
    "We've already loaded our data and split it into training and validation datasets. However, we need to do some further data preparation so that our data will work correctly with TensorFlow. Specifically, we need to set the data type of our features to 32-bit floating point numbers, and specify that the labels represent categorical classes rather than numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d556d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:16:58.365610Z",
     "iopub.status.busy": "2022-03-12T18:16:58.363047Z",
     "iopub.status.idle": "2022-03-12T18:16:58.368733Z",
     "shell.execute_reply": "2022-03-12T18:16:58.366321Z",
     "shell.execute_reply.started": "2022-03-12T17:38:21.991627Z"
    },
    "papermill": {
     "duration": 0.39051,
     "end_time": "2022-03-12T18:16:58.368896",
     "exception": false,
     "start_time": "2022-03-12T18:16:57.978386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready...\n"
     ]
    }
   ],
   "source": [
    "# Set data types for float features\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Set the data types for categorical variables\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "print('Ready...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c559c",
   "metadata": {
    "papermill": {
     "duration": 0.20147,
     "end_time": "2022-03-12T18:16:58.817622",
     "exception": false,
     "start_time": "2022-03-12T18:16:58.616152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define a neural network\n",
    "\n",
    "Now we're ready to define our neural network. In this case, we'll create a network that consists of 3 fully-connected layers:\n",
    "* An input layer that receives an input value for each feature (in this case, the four penguin measurements) and applies a *ReLU* activation function.\n",
    "* A hidden layer that receives ten inputs and applies a *ReLU* activation function.\n",
    "* An output layer that uses a *SoftMax* activation function to generate an output for each penguin species (which represent the classification probabilities for each of the three possible penguin species). Softmax functions produce a vector with probability values that sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156045c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:16:59.296646Z",
     "iopub.status.busy": "2022-03-12T18:16:59.295817Z",
     "iopub.status.idle": "2022-03-12T18:16:59.757351Z",
     "shell.execute_reply": "2022-03-12T18:16:59.751412Z",
     "shell.execute_reply.started": "2022-03-12T17:59:11.218135Z"
    },
    "papermill": {
     "duration": 0.737647,
     "end_time": "2022-03-12T18:16:59.757558",
     "exception": false,
     "start_time": "2022-03-12T18:16:59.019911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-12 18:16:59.427389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-03-12 18:16:59.444136: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                50        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a classifier network\n",
    "hl = 10 # Number of hidden layer nodes\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(hl, input_dim=len(features), activation='relu'))\n",
    "model.add(Dense(hl, input_dim=hl, activation='relu'))\n",
    "model.add(Dense(len(penguin_classes), input_dim=hl, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b166d",
   "metadata": {
    "papermill": {
     "duration": 0.297658,
     "end_time": "2022-03-12T18:17:00.356415",
     "exception": false,
     "start_time": "2022-03-12T18:17:00.058757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "\n",
    "To train the model, we need to repeatedly feed the training values forward through the network, use a loss function to calculate the loss, use an optimizer to backpropagate the weight and bias value adjustments, and validate the model using the test data we withheld.\n",
    "\n",
    "To do this, we'll apply an Adam optimizer to a categorical cross-entropy loss function iteratively over 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c79748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T18:17:00.735845Z",
     "iopub.status.busy": "2022-03-12T18:17:00.735012Z",
     "iopub.status.idle": "2022-03-12T18:17:21.631756Z",
     "shell.execute_reply": "2022-03-12T18:17:21.632562Z",
     "shell.execute_reply.started": "2022-03-12T18:14:29.434456Z"
    },
    "papermill": {
     "duration": 21.091788,
     "end_time": "2022-03-12T18:17:21.632748",
     "exception": false,
     "start_time": "2022-03-12T18:17:00.540960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 1s 4ms/step - loss: 23.8942 - accuracy: 0.1912 - val_loss: 11.2900 - val_accuracy: 0.2165\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 5.0865 - accuracy: 0.2142 - val_loss: 1.5626 - val_accuracy: 0.1752\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.2712 - accuracy: 0.2727 - val_loss: 1.1591 - val_accuracy: 0.3139\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.1068 - accuracy: 0.3814 - val_loss: 1.0832 - val_accuracy: 0.4453\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.0432 - accuracy: 0.4598 - val_loss: 1.0545 - val_accuracy: 0.4574\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.0258 - accuracy: 0.5183 - val_loss: 1.0425 - val_accuracy: 0.4842\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.0118 - accuracy: 0.5601 - val_loss: 1.0315 - val_accuracy: 0.5547\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 1.0008 - accuracy: 0.5998 - val_loss: 1.0189 - val_accuracy: 0.6156\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.9920 - accuracy: 0.6207 - val_loss: 1.0050 - val_accuracy: 0.6521\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.9762 - accuracy: 0.6667 - val_loss: 0.9911 - val_accuracy: 0.6618\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.9639 - accuracy: 0.6949 - val_loss: 0.9781 - val_accuracy: 0.6813\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.9394 - accuracy: 0.6803 - val_loss: 0.8758 - val_accuracy: 0.6350\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.7998 - accuracy: 0.7200 - val_loss: 0.7991 - val_accuracy: 0.7129\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.7341 - accuracy: 0.7450 - val_loss: 0.7380 - val_accuracy: 0.7202\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.6758 - accuracy: 0.7764 - val_loss: 0.6802 - val_accuracy: 0.7421\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.7847 - val_loss: 0.6292 - val_accuracy: 0.7470\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.5758 - accuracy: 0.7973 - val_loss: 0.5871 - val_accuracy: 0.7567\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7962 - val_loss: 0.5660 - val_accuracy: 0.7786\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.8297 - val_loss: 0.5272 - val_accuracy: 0.7786\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.8213 - val_loss: 0.4849 - val_accuracy: 0.7713\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.8318 - val_loss: 0.4676 - val_accuracy: 0.7640\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8422 - val_loss: 0.4518 - val_accuracy: 0.7689\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8527 - val_loss: 0.4383 - val_accuracy: 0.8054\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8662 - val_loss: 0.3915 - val_accuracy: 0.8224\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8746 - val_loss: 0.3924 - val_accuracy: 0.8808\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8934 - val_loss: 0.3521 - val_accuracy: 0.8710\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.9080 - val_loss: 0.3362 - val_accuracy: 0.8759\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.9195 - val_loss: 0.3216 - val_accuracy: 0.9002\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.9310 - val_loss: 0.3074 - val_accuracy: 0.8929\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.9363 - val_loss: 0.2890 - val_accuracy: 0.9075\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.9446 - val_loss: 0.2870 - val_accuracy: 0.8662\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9394 - val_loss: 0.2752 - val_accuracy: 0.8832\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9457 - val_loss: 0.2491 - val_accuracy: 0.9148\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9509 - val_loss: 0.2343 - val_accuracy: 0.9367\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9530 - val_loss: 0.2233 - val_accuracy: 0.9489\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.9624 - val_loss: 0.2061 - val_accuracy: 0.9635\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.9561 - val_loss: 0.2004 - val_accuracy: 0.9562\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9655 - val_loss: 0.1851 - val_accuracy: 0.9732\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1618 - accuracy: 0.9718 - val_loss: 0.1749 - val_accuracy: 0.9757\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.9687 - val_loss: 0.1627 - val_accuracy: 0.9684\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9707 - val_loss: 0.1555 - val_accuracy: 0.9708\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1393 - accuracy: 0.9666 - val_loss: 0.1717 - val_accuracy: 0.9465\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1286 - accuracy: 0.9781 - val_loss: 0.1403 - val_accuracy: 0.9805\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9749 - val_loss: 0.1313 - val_accuracy: 0.9781\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9749 - val_loss: 0.1223 - val_accuracy: 0.9805\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9801 - val_loss: 0.1165 - val_accuracy: 0.9830\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.1034 - accuracy: 0.9833 - val_loss: 0.1173 - val_accuracy: 0.9732\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.0971 - accuracy: 0.9833 - val_loss: 0.1118 - val_accuracy: 0.9805\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9833 - val_loss: 0.1018 - val_accuracy: 0.9830\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 0s 2ms/step - loss: 0.0894 - accuracy: 0.9822 - val_loss: 0.1046 - val_accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters for optimizers\n",
    "learning_rate = 0.001\n",
    "opt = optimizers.Adam(lr = learning_rate)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
    "num_epochs = 50\n",
    "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=10, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 103.705785,
   "end_time": "2022-03-12T18:17:25.351922",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-12T18:15:41.646137",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
