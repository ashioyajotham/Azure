// A Convolutional Neural Network (CNN) for image classification is typically composed of multiple layers that extract features, and then use a final fully connected layer to classify images based on these features.

Conceptually, this neural network consists of two distinct sets of layers:

        * A set of layers from the base model that perform feature extraction.
        * A fully connected layer that takes the extracted features and uses them for class prediction.

*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.


You are creating a deep neural network to train a classification model that predicts to which of three classes an observation belongs based on 10 numeric features. 

    The output layer should contain three nodes
    
 
You are training a deep neural network. You configure the training process to use 50 epochs. What effect does this configuration have?

           The entire training dataset is passed through the network 50 times
           
           
You are creating a deep neural network. You increase the Learning Rate parameter. What effect does this setting have?
       Larger adjustments are made to weight values during backpropagation
       
     
You are creating a convolutional neural network. You want to reduce the size of the feature maps that are generated by a convolutional layer. What should you do?

          Add a pooling layer after the convolutional layer
       
