## Train the Clustering Model

// There are several algorithms used for clustering the most common being KMeans which consists of the following steps;
  i) The feature values are vectorized to define n-dimensional coordinates (where n is the number of features). 
     In the flower example, we have two features (number of petals and number of leaves), so the feature vector has two coordinates 
       that we can use to conceptually plot the data points in two-dimensional space.
       
   ii) You decide how many clusters you want to use to group the flowers, and call this value k. 
       For example, to create three clusters, you would use a k value of 3. Then k points are plotted at random coordinates. 
       These points will ultimately be the center points for each cluster, so they're referred to as centroids.
       
   iii) Each data point (in this case flower) is assigned to its nearest centroid.
   iv) Each centroid is moved to the center of the data points assigned to it based on the mean distance between the points.
   v) After moving the centroid, the data points may now be closer to a different centroid, so the data points are reassigned to clusters based on the new closest centroid.
  vi) The centroid movement and cluster reallocation steps are repeated until the clusters become stable or a pre-determined maximum number of iterations is reached.
  
  
  ## Hierarchical Clustering
  //Another type of clustering is known as Hierarchical Clustering. 
  //Hierarchical clustering is another type of clustering algorithm in which clusters themselves belong to a larger group, which belong to even larger groups, and so on.
  //The result is that data points can be clusters in differing degrees of precision: with a large number of very small and precise groups, or a small number of larger groups.
  //Hierarchical clustering is useful for not only breaking data into groups, but understanding the relationships between these groups. A major advantage of hierarchical clustering is that it 
    does not require the number of clusters to be defined in advance, and can sometimes provide more interpretable results than non-hierarchical approaches. 
    The major drawback is that these approaches can take much longer to compute than simpler approaches and sometimes are not suitable for large datasets.
  
